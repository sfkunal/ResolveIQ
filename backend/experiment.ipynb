{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Request to Block Cases from Specific Region\n",
      "Description: I need to stop receiving cases from the APAC region as I don't have the language capabilities to handle them effectively. Is there a way to update my assignment rules to exclude these cases?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_tickets():\n",
    "    with open('support_tickets.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def get_ticket(index):\n",
    "    with open('support_tickets.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data['tickets'][index]\n",
    "\n",
    "def stringify_ticket(ticket):\n",
    "    return f\"Title: {ticket['title']}\\nDescription: {ticket['description']}\"\n",
    "\n",
    "ticket = get_ticket(1)\n",
    "print(stringify_ticket(ticket))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import markdown\n",
    "\n",
    "\n",
    "# def load_knowledge_base():\n",
    "#     with open('knowledge_wiki.md', 'r') as file:\n",
    "#         kb_content = file.read()\n",
    "    \n",
    "#     kb_text = markdown.markdown(kb_content)\n",
    "#     # text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     #     chunk_size=2000,\n",
    "#     #     chunk_overlap=50,\n",
    "#     #     length_function=len,\n",
    "#     # )\n",
    "    \n",
    "#     # chunks = text_splitter.split_text(kb_text)\n",
    "#     chunks = [chunk.strip() for chunk in kb_text.split('<h3>') if chunk.strip()]\n",
    "\n",
    "#     print(chunks)\n",
    "#     embeddings = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "#     vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "#     return vectorstore\n",
    "\n",
    "def load_knowledge_base():\n",
    "    with open('knowledge_wiki.md', 'r') as file:\n",
    "        kb_content = file.read()\n",
    "    \n",
    "    # Split content on ### headers\n",
    "    chunks = [chunk.strip() for chunk in kb_content.split('###') if chunk.strip()]\n",
    "    \n",
    "    # Convert markdown to HTML, then strip HTML tags to get clean text\n",
    "    clean_chunks = []\n",
    "    for chunk in chunks:\n",
    "        html = markdown.markdown(chunk)\n",
    "        # Basic HTML tag removal (you might want to use a more robust HTML parser)\n",
    "        clean_text = html.replace('<strong>', '').replace('</strong>', '') \\\n",
    "                        .replace('<p>', '').replace('</p>', '') \\\n",
    "                        .replace('<h3>', '').replace('</h3>', '') \\\n",
    "                        .replace('<ol>', '').replace('</ol>', '') \\\n",
    "                        .replace('<li>', '').replace('</li>', '')\n",
    "        clean_chunks.append(clean_text)\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_texts(\n",
    "        clean_chunks,\n",
    "        embeddings,\n",
    "        metadatas=[{\"chunk_id\": i, \"source\": \"knowledge_wiki\"} for i in range(len(clean_chunks))]\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def find_relevant_knowledge(ticket_string, top_k=3):\n",
    "    vectorstore = load_knowledge_base()\n",
    "    relevant_chunks = vectorstore.similarity_search(ticket_string, k=top_k)\n",
    "    return [doc.page_content for doc in relevant_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vj/w8n90pdn5nj0vq3c9w7q0hmc0000gn/T/ipykernel_72739/1151345685.py:46: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/Users/sfkunal/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/sfkunal/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x3100fa7d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_knowledge_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket:  Title: Time Zone Update Required\n",
      "Description: I recently relocated to the EST time zone, but my working hours are still showing in PST. Need assistance updating my profile to reflect the correct time zone so my availability is properly tracked.\n",
      "\n",
      "Relevant knowledge base sections:\n",
      "\n",
      "Chunk 1:\n",
      "Updating Time Zone Settings\n",
      "To update your working hours and reflect the correct time zone:\n",
      "1. Go to Profile Settings: Locate the \"Time Zone\" section.\n",
      "2. Select the Correct Time Zone: Choose your current time zone from the dropdown.\n",
      "3. Verify Your Schedule: Ensure that your work hours align with the new time zone.\n",
      "4. Save Changes: Refresh the system and test by checking availability.\n",
      "\n",
      "Chunk 2:\n",
      "Troubleshooting Auto Assignment Issues\n",
      "If you are not receiving auto assignments despite being marked as \"Available,\" follow these steps:\n",
      "1. Verify System Status: Check for ongoing outages or maintenance in the assignment system.\n",
      "2. Check Queue Configuration: Ensure your queue is properly configured and there are no conflicting filters.\n",
      "3. Manually Refresh Status: Toggle your availability off and on again.\n",
      "4. Contact Support: If the issue persists, reach out to the system administrator for further investigation.\n",
      "\n",
      "Chunk 3:\n",
      "Resolving VPN Connectivity Issues\n",
      "If you cannot connect to the company VPN:\n",
      "1. Restart Your Device and Network.\n",
      "2. Verify VPN Credentials.\n",
      "3. Check for Firewall Restrictions.\n",
      "4. Contact IT Support If Issue Persists.\n"
     ]
    }
   ],
   "source": [
    "ticket = get_ticket(2)\n",
    "ticket_string = stringify_ticket(ticket)\n",
    "print(\"Ticket: \", ticket_string)\n",
    "relevant_knowledge = find_relevant_knowledge(ticket_string)\n",
    "print(\"\\nRelevant knowledge base sections:\")\n",
    "for i, chunk in enumerate(relevant_knowledge, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A programmer walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and Schr√∂dinger's cat?\" The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat_model = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Tell me a short joke about programming.\")\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
